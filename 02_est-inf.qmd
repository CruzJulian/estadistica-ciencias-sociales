---
title: "Estadística II"
subtitle: "Introducción a la inferencia estadística"
author: "CruzJulián"

lang: "es-co"
---

```{r, echo = FALSE}

# Falta:
#   
#   Pasar todo esto al formato de openintrostatistics

"http://www.bnm.me.gov.ar/giga1/documentos/EL001858.pdf"


```

# Antecedentes

## Presentación

### Descripción del curso

Estadística 2: Estadística Inferencial para el seminario de Técnicas Especiales de Investigación II, es un curso muy rápido e introductorio que se ofrece a los estudiantes como parte del conjunto de herramientas que hacen parte de la programación completa de la materia. El contenido de las sesiones se plantea como introducción y con el objetivo de reforzar temas de estadística a los estudiantes sobre los conceptos de pruebas de estimación y pruebas de hipótesis.

### Justificación

Técnicas Especiales de Investigación II es la segunda de las cuatro materias del ciclo que orienta a los estudiantes en el desarrollo de sus proyectos de grado. Dado que la única estadística que se ofrece en los programas de Ciencias Sociales de Universidad Externado de Colombia se da en primer semestre, y los asistentes al seminario son de quinto y sexto semestre, se ve la necesidad de reforzar los temas de estadística inferencial para que los estudiantes vean la posibilidad de soportar sus investigaciones con datos tratados estadísticamente y dado el carácter investigativo de la Facultad.

## Consideraciones

El curso está lleno de contenidos prácticos y ligeros que no exigen a los estudiantes mayores conocimientos o destrezas sobre los temas ni la realización de tareas o trabajos profundos por fuera del aula. Más bien es impartir conocimiento y entregar herramientas básicas de estadística como apoyo a las investigaciones y documentos exigidos como proyectos de investigación.

## Objetivo General

Ofrecer herramientas de estadística inferencial a los asistentes al seminario de Técnicas Especiales de Investigación II, como repaso de la estadística ofrecida en primer semestre, para que se vea la pertinencia y necesidad de incorporar procesamiento y análisis estadístico a los proyectos de investigación.

## Objetivos Específicos

-   Generar los conceptos y conocimientos sobre estimación y pruebas de hipótesis.
-   Adelantar talleres prácticos que hagan evidentes los conceptos presentados.
-   Orientar la aplicación de estos conceptos en los proyectos particulares de cada estudiante.

## Metodología

Curso magistral con talleres prácticos en donde se involucren todos los asistentes mediante el desarrollo de un taller y exposición de resultados.

## Material

[OpenIntro Statistics](https://openintro.org/book/os/)

http://www.bnm.me.gov.ar/giga1/documentos/EL001858.pdf

```         
@book{132224,
    author = {Kelmansky, Diana M. and Kirschenbaum, Juan Manuel},
    title = {Estadística para todos :},
    publisher = {Instituto Nacional de Educación Tecnológica INET,},
    year = {2009.},
    series = {Las ciencias naturales y la matemática},
    address = {Buenos Aires :},
    edition = {1ª ed.},
    url = {http://www.bnm.me.gov.ar/giga1/documentos/EL001858.pdf}
}


```

# Construyendo un marco epistemológico para la inferencia estadística

Para comprender cómo surgió la inferencia estadística y en qué se fundamenta, es necesario entender primero las bases epistemológicas de la ciencia. Los métodos que hoy empleamos en estadística, especialmente en inferencia, tienen raíces profundas en la filosofía clásica y el desarrollo del método científico. A través de la deducción, la inducción y otras formas de razonamiento, los científicos han perfeccionado métodos para obtener conocimientos que sean precisos y replicables.

## Los antiguos griegos y la deducción

### Sócrates y la mayéutica

Sócrates fue uno de los primeros filósofos en enfatizar la importancia del cuestionamiento como herramienta para alcanzar la verdad. Su método, la **mayéutica**, consistía en formular preguntas para ayudar a su interlocutor a descubrir conocimientos por sí mismo, partiendo de sus propias creencias y explorando las inconsistencias en sus respuestas. A través del diálogo y la introspección, Sócrates buscaba llevar a los demás hacia una mejor comprensión de conceptos abstractos como la justicia, la verdad y el bien. Esta metodología sentó las bases para el pensamiento crítico, un pilar fundamental en la ciencia moderna.

### Platón y la dialéctica

Platón, discípulo de Sócrates, amplió la mayéutica y formuló la **dialéctica** como un método para alcanzar conocimientos más profundos mediante la confrontación de ideas opuestas. A través del diálogo y la tensión entre las tesis y antítesis, Platón creía que se podía llegar a la síntesis, es decir, a una comprensión superior y más completa de la realidad. Este método dialéctico influyó en el desarrollo de sistemas de lógica y pensamiento analítico que aún sustentan la base epistemológica de la ciencia.

### Aristóteles y la lógica

Aristóteles sistematizó la lógica como un método de razonamiento para llegar a conclusiones válidas a partir de premisas establecidas. En sus obras, como el **Organon**, formalizó el uso de la lógica deductiva y desarrolló una metodología para analizar y entender los principios subyacentes de los fenómenos. La lógica aristotélica no solo sentó las bases para el razonamiento científico, sino que también proporcionó las herramientas para la creación de sistemas de clasificación y el desarrollo de conceptos abstractos en la ciencia y las matemáticas.

### El silogismo

Uno de los aportes más significativos de Aristóteles a la epistemología es el **silogismo**, una forma de razonamiento deductivo que permite derivar conclusiones a partir de dos o más premisas. El silogismo establece que si las premisas son verdaderas, la conclusión necesariamente debe serlo. Este tipo de razonamiento deductivo es un modelo de inferencia lógica que ha servido de base para el desarrollo de sistemas matemáticos y estadísticos. Un ejemplo clásico sería:

-   Todos los hombres son mortales.\
-   Sócrates es un hombre.\
-   Por lo tanto, Sócrates es mortal.

Un ejemplo del uso del mecanismo deductivo es el segundo libro más editado de la historia, *Los Elementos* de Euclides, que organiza el conocimiento geométrico mediante un sistema axiomático. En este texto, Euclides parte de unos pocos postulados y axiomas fundamentales, a partir de los cuales deduce rigurosamente una serie de teoremas y proposiciones. Este enfoque deductivo no solo demostró la efectividad de la lógica en las matemáticas, sino que también influyó profundamente en la metodología científica, sirviendo de modelo para estructurar el conocimiento de manera lógica y coherente.

Otros ejemplos son *Ética demostrada según el orden geométrico* de Spinoza y *Philosophiæ Naturalis Principia Mathematica* de Isaac Newton. En *Ética*, Spinoza estructura su filosofía siguiendo el estilo geométrico de Euclides, utilizando definiciones, axiomas y proposiciones para desarrollar sus ideas sobre la naturaleza de Dios, la mente y la moralidad. Por su parte, en *Principia Mathematica*, Newton aplica un razonamiento deductivo para establecer las leyes del movimiento y la gravitación universal, partiendo de principios fundamentales y llegando a conclusiones que explican fenómenos físicos observables. Estos textos muestran cómo el método deductivo ha sido un pilar para avanzar en diversas disciplinas, desde la filosofía hasta la física.

## El *Novum Organum* de Bacon

### La deducción contra la inducción

En el siglo XVII, Francis Bacon introdujo un enfoque revolucionario en su obra *Novum Organum*, en la que defendía la **inducción** como método para el conocimiento científico. Este enfoque rompió con la tradición aristotélica de deducción estricta, proponiendo que, en lugar de solo partir de premisas generales, los científicos deberían observar y analizar los fenómenos específicos para, a partir de ellos, generalizar leyes y principios.

-   **La deducción**: es un proceso de razonamiento que va de lo general a lo particular. Parte de leyes o teorías ya establecidas y aplica esas premisas para llegar a conclusiones específicas. La deducción asegura conclusiones válidas si las premisas son verdaderas, pero no permite descubrir nuevas leyes o principios.

-   **La inducción**: es el proceso de observación de casos particulares para generar conclusiones generales o teorías. En la inducción, el conocimiento se construye a partir de patrones observados en la realidad, permitiendo la creación de nuevas hipótesis y teorías. Sin embargo, este método no garantiza la certeza absoluta de sus conclusiones, ya que estas son probabilísticas y dependen de la representatividad de los datos.

El trabajo de Bacon es fundamental porque sentó las bases para una ciencia basada en la observación empírica, un enfoque que siglos más tarde sería crucial en la inferencia estadística.

## Actividad en clase

**¿Qué es un cisne negro?**

El concepto de "cisne negro" se refiere a eventos altamente improbables e impredecibles, pero con un gran impacto cuando ocurren. La expresión fue popularizada por el filósofo Nassim Nicholas Taleb y subraya la limitación de los métodos inductivos, ya que una amplia observación de cisnes blancos no garantiza que no existan cisnes negros. Este concepto es clave para entender los límites de la inferencia estadística y la probabilidad, pues resalta la posibilidad de eventos fuera de nuestras expectativas basadas en observaciones pasadas.

## Fisher, Neyman, Pearson

### Las reglas para hacer inducción

Ronald A. Fisher, Jerzy Neyman y Egon Pearson fueron fundamentales para estructurar las **reglas para hacer inducción** en el contexto de la estadística moderna. Su trabajo permitió la formalización de métodos inferenciales que ayudan a generalizar conclusiones a partir de muestras. Estas reglas establecen la estructura de las pruebas de hipótesis y la generación de intervalos de confianza, permitiendo a los científicos tomar decisiones con base en evidencia empírica.

### Generación de conocimiento a partir de datos

Fisher, Neyman y Pearson desarrollaron metodologías para derivar conocimiento a partir de datos de manera rigurosa, incorporando conceptos como la probabilidad y el error estadístico. A través de la estadística inferencial, lograron definir un proceso sistemático para probar hipótesis, medir la incertidumbre y proporcionar intervalos de confianza, contribuyendo significativamente a las ciencias experimentales y sociales.

### Inferencia estadística

La inferencia estadística surgió en el siglo XX como una disciplina clave en la estadística, impulsada por la necesidad de tomar decisiones informadas a partir de datos. Su desarrollo fue influenciado por figuras como Ronald A. Fisher, Jerzy Neyman y Egon Pearson, quienes sentaron las bases de los métodos inferenciales que permiten generalizar conclusiones de una muestra a una población más amplia. Fisher introdujo conceptos fundamentales como el "p-valor" y la prueba de hipótesis, mientras que Neyman y Pearson formalizaron la teoría de pruebas con su trabajo sobre errores tipo I y II y la formulación de intervalos de confianza. La inferencia estadística se consolidó rápidamente en diversas áreas científicas, desde la biología y la medicina hasta las ciencias sociales y económicas, transformando la manera en que los investigadores validan teorías y estiman parámetros poblacionales. A lo largo del tiempo, esta área ha evolucionado, incorporando herramientas computacionales y métodos bayesianos que amplían las posibilidades de análisis en contextos de datos complejos y grandes volúmenes de información.

Aquí tienes el contenido desarrollado para las secciones del segundo capítulo sobre estimación, siguiendo la estructura que proporcionaste:

------------------------------------------------------------------------

# Estimación

## Conceptos iniciales

Iniciamos por los conceptos que son fundamentales para entender la estimación en inferencia estadística. Estos conceptos ayudan a diferenciar entre la población y la muestra, así como a identificar las características numéricas que se desea estimar.

### Parámetro

Un **parámetro** es un valor numérico que describe una característica específica de una población completa. Ejemplos incluyen la media poblacional (μ), la varianza poblacional (σ²) o la proporción poblacional (P). Dado que los parámetros se refieren a poblaciones enteras, son constantes, pero, en la práctica, generalmente son desconocidos. Por lo tanto, se necesita recurrir a métodos estadísticos para estimarlos a partir de muestras.

### Estimador

Un **estimador** es una regla o fórmula utilizada para aproximar un parámetro poblacional a partir de los datos muestrales. Un estimador es una variable aleatoria y, por lo tanto, puede variar de una muestra a otra. Los estimadores se utilizan para hacer inferencias sobre la población, y algunos ejemplos comunes incluyen la media muestral $\bar{x}$ y la proporción muestral $\hat{p}$. La calidad de un estimador se evalúa en función de sus propiedades, como la consistencia y la unbiasedness (inexistencia de sesgo).

### Muestra

Una **muestra** es un subconjunto de elementos extraídos de una población. La muestra se utiliza para realizar estimaciones sobre la población completa sin necesidad de medir a todos sus miembros. El tamaño y la calidad de la muestra son fundamentales para asegurar que las inferencias realizadas sean precisas y representativas. Una muestra puede ser aleatoria, estratificada o por conveniencia, dependiendo del método utilizado para su selección.

### Población

La **población** se refiere al conjunto completo de individuos, elementos o unidades que comparten una característica particular que se está estudiando. Por ejemplo, si estamos interesados en el ingreso anual de todos los empleados de una empresa, la población sería todos los empleados. En la inferencia estadística, es crucial definir claramente la población para poder aplicar métodos de muestreo adecuados y realizar estimaciones precisas.

## Estimación puntual

La **estimación puntual** se refiere al uso de un solo valor, calculado a partir de la muestra, para estimar un parámetro desconocido de la población. Este enfoque proporciona una respuesta única a la pregunta de interés, pero no informa sobre la precisión de la estimación. Por ejemplo, la media muestral $\bar{x}$ es una estimación puntual de la media poblacional (μ).

-   **Media muestral:** Se toma una muestra de los ingresos anuales de 100 empleados de una empresa. La media de estos ingresos en la muestra es de \$50,000. Este valor se utiliza como una **estimación puntual** de la media de los ingresos de toda la población de empleados.

-   **Proporción muestral:** En una encuesta realizada a 500 personas, 320 afirman que prefieren trabajar desde casa. La proporción muestral es $\hat{p} = \frac{320}{500} = 0.64$, lo cual es una **estimación puntual** de la proporción real de personas que prefieren el trabajo remoto en la población general.

-   **Varianza muestral:** Una muestra de 50 estudiantes tiene una varianza de puntajes en un examen de 16 puntos cuadrados. Este valor se utiliza como una **estimación puntual** de la varianza de los puntajes en la población completa de estudiantes.

### Estimadores y parámetros

Los estimadores son utilizados para estimar parámetros poblacionales. Algunos ejemplos incluyen:

-   **Media muestral** $\bar{x}$ para estimar la media poblacional (μ).
-   **Varianza muestral** $s^2$ para estimar la varianza poblacional (σ²).
-   **Proporción muestral** $\hat{p}$ para estimar la proporción poblacional (P).
-   **Correlación de Pearson** $\hat{\rho}$ para estimar la correlacción poblacional $\rho$.

### Aplicación interactiva

Para ilustrar la idea de estimación puntual, podemos estimar el número π utilizando simulaciones. Aquí tienes un recurso que permite interactuar con este concepto:

[Estimación puntual](https://seeing-theory.brown.edu/frequentist-inference/)

### Ley de los Grandes Números

La **Ley de los Grandes Números** es un principio fundamental en la teoría de la probabilidad que establece que, a medida que aumenta el tamaño de una muestra, la media muestral $\bar{x}$ se acercará a la media poblacional (μ) de la población de la que se extrajo la muestra. Esta ley se basa en la idea de que las fluctuaciones aleatorias tienden a cancelarse entre sí en muestras más grandes, lo que resulta en una estimación más precisa del parámetro poblacional.

Existen dos versiones de la Ley de los Grandes Números:

1.  **Ley débil de los grandes números**: Establece que, para cualquier valor ε positivo, la probabilidad de que la media muestral se desvíe de la media poblacional en más de ε tiende a cero a medida que el tamaño de la muestra (n) aumenta. Es decir, la media muestral se convierte en un estimador consistente de la media poblacional.

2.  **Ley fuerte de los grandes números**: Afirmación más fuerte que la versión débil, establece que la media muestral converge casi seguramente a la media poblacional a medida que n tiende al infinito. Esto significa que, con una probabilidad de 1, la media muestral se aproximará a la media poblacional a medida que se tomen más y más muestras.

La Ley de los Grandes Números es fundamental en estadística y asegura que los resultados obtenidos de las muestras se volverán más representativos de la población a medida que se aumente el tamaño de la muestra, lo que permite realizar inferencias más confiables sobre la población completa.

### Consistencia

Un **estimador puntual** es **consistente** si, a medida que el tamaño de la muestra aumenta, la estimación se aproxima cada vez más al verdadero parámetro poblacional. En otras palabras, un estimador consistente converge en probabilidad al parámetro que se está estimando. Esta propiedad es crucial porque asegura que con muestras más grandes, nuestras estimaciones se vuelven más precisas y confiables, reduciendo la variabilidad y el error de estimación.

## Estimación por intervalo

La **estimación por intervalo** proporciona un rango de valores dentro del cual se espera que se encuentre el parámetro poblacional con un cierto nivel de confianza. A diferencia de una estimación puntual, que ofrece un único valor, el intervalo de confianza incluye una medida de la incertidumbre asociada con la estimación. Esto permite a los investigadores entender no solo qué valor se estima, sino también la precisión y confiabilidad de dicha estimación.

### Precisión y Exactitud


**Precisión** y **exactitud** son dos conceptos fundamentales en la estadística y la investigación que se utilizan para evaluar la calidad de las estimaciones y mediciones. Aunque a menudo se utilizan de manera intercambiable en el lenguaje cotidiano, tienen significados distintos en el contexto estadístico.


#### Precisión

La **precisión** se refiere a la consistencia y reproducibilidad de las mediciones. Un conjunto de datos es preciso si las mediciones son cercanas entre sí, independientemente de si son correctas o no. En otras palabras, la precisión indica cuán dispersos están los valores en relación con la media. Un alto grado de precisión significa que las mediciones tienden a agruparse en torno a un valor central.

- **Ejemplo de Precisión:** Supongamos que un grupo de científicos mide la temperatura de un líquido en tres ocasiones y obtiene los siguientes valores: 22.1°C, 22.0°C y 22.2°C. Aunque la medición puede no ser la temperatura real del líquido, los valores son consistentes entre sí, lo que indica alta precisión.

#### Exactitud

La **exactitud**, por otro lado, se refiere a cuán cerca está una medición del valor verdadero o del objetivo. Una medición es exacta si se aproxima al valor real. En este caso, la exactitud evalúa la validez de los datos en relación con la realidad.

- **Ejemplo de Exactitud:** Siguiendo el mismo ejemplo anterior, si la temperatura real del líquido es de 23.0°C y las mediciones obtenidas fueron 22.1°C, 22.0°C y 22.2°C, podemos decir que las mediciones son imprecisas a pesar de ser consistentes, ya que están lejos del valor verdadero.

#### Diferencias entre Precisión y Exactitud


| Concepto      | Definición                                    | Enfoque                           | Ejemplo                                                                                                                                          |
|---------------|----------------------------------------------|-----------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|
| Precisión     | Consistencia y reproducibilidad de mediciones | Distribución de datos             | Mediciones: 10.1, 10.0, 10.2 (altamente precisas pero no exactas si el valor verdadero es 11.0)                                              |
| Exactitud     | Cercanía de las mediciones al valor verdadero | Proximidad al valor verdadero     | Mediciones: 11.0, 11.1, 11.2 (altamente exactas si el valor verdadero es 11.0, aunque pueden no ser precisas si los valores no son consistentes) |


La precisión y exactitud son cruciales en la estadística y en el diseño de investigaciones. Una alta precisión en los resultados es deseable, pero no es suficiente por sí sola. Si los datos son precisos pero no exactos, las conclusiones extraídas pueden ser engañosas y llevar a decisiones erróneas.

1. **Diseño de Estudios:** Durante el diseño de estudios, los investigadores deben asegurarse de que las herramientas de medición sean tanto precisas como exactas para obtener datos confiables.
  
2. **Interpretación de Resultados:** La evaluación de la precisión y exactitud de los datos ayuda a los investigadores a entender la confiabilidad de sus estimaciones y a interpretar correctamente los resultados.

3. **Mejora de Métodos:** Comprender la diferencia entre estos conceptos permite a los investigadores identificar áreas de mejora en sus métodos de recopilación de datos, ajustando su enfoque para aumentar tanto la precisión como la exactitud.


Los conceptos de precisión y exactitud están intrínsecamente relacionados con los intervalos de confianza, ya que estos intervalos son una herramienta estadística diseñada para expresar la incertidumbre en torno a una estimación puntual. Un intervalo de confianza proporciona un rango de valores dentro del cual se espera que se encuentre el parámetro poblacional verdadero, lo que refleja la **exactitud** de la estimación. Si el intervalo es estrecho, indica una alta precisión en las mediciones, lo que sugiere que repetidas mediciones generarían resultados consistentes. Sin embargo, si el intervalo de confianza incluye valores muy alejados del verdadero parámetro, sugiere que la estimación puede no ser precisa. Por lo tanto, un intervalo de confianza bien construido no solo comunica la variabilidad de los datos, sino que también integra las nociones de precisión y exactitud en la evaluación de la validez de los resultados estadísticos.

### Ejemplo introductorio

#### Intervalo de confianza para la media

Para calcular un intervalo de confianza para la media de una población, se puede utilizar la distribución t de Student o la distribución normal, dependiendo del tamaño de la muestra y de si se conoce la varianza poblacional.

-   **Ejemplo:** Supongamos que un investigador quiere estimar la media de la altura de los estudiantes de una universidad. Toma una muestra aleatoria de 30 estudiantes y calcula que la media muestral es de 1.70 metros con una desviación estándar de 0.10 metros. Dado que el tamaño de la muestra es pequeño, se utiliza la distribución t de Student. Si se quiere un nivel de confianza del 95%, el intervalo de confianza se calcula de la siguiente manera:

$$IC = \bar{x} \pm t_{\alpha/2} \left(\frac{s}{\sqrt{n}}\right)$$

Donde $t_{\alpha/2}$ es el valor crítico de t correspondiente al nivel de confianza del 95% y 29 grados de libertad (n-1), (s) es la desviación estándar muestral y (n) es el tamaño de la muestra. Supongamos que $t_{0.025} \approx 2.045$ para este caso:

$$IC = 1.70 \pm 2.045 \left(\frac{0.10}{\sqrt{30}}\right) \approx 1.70 \pm 0.374$$

El intervalo de confianza sería aproximadamente (1.63, 1.77). Esto significa que el investigador puede estar 95% seguro de que la media de altura de todos los estudiantes está entre 1.63 y 1.77 metros.

#### Intervalo de confianza para una proporción

El **intervalo de confianza para una proporción** se calcula utilizando la proporción muestral $\hat{p}$ y el error estándar de la proporción. Se utiliza la distribución normal, ya que se asume que la proporción muestral sigue una distribución normal cuando el tamaño de muestra es suficientemente grande.

-   **Ejemplo:** Imaginemos que en una encuesta a 500 personas, 320 afirman que prefieren trabajar desde casa. La proporción muestral es:

$$ \hat{p} = \frac{320}{500} = 0.64 $$

El intervalo de confianza se puede calcular de la siguiente manera:

$$ IC = \hat{p} \pm z_{\alpha/2} \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}} $$

Si elegimos un nivel de confianza del 95%, $z_{0.025} \approx 1.96$:

$$ IC = 0.64 \pm 1.96 \sqrt{\frac{0.64 \times (1 - 0.64)}{500}} \approx 0.64 \pm 0.045 $$

Esto resulta en un intervalo de confianza de (0.595, 0.685). Por lo tanto, el investigador puede estar 95% seguro de que la proporción real de personas que prefieren trabajar desde casa en la población general está entre el 59.5% y el 68.5%.

#### Intervalo de confianza para la varianza

El **intervalo de confianza para la varianza** se calcula utilizando la distribución chi-cuadrado. La varianza muestral (s²) es el estimador puntual, y el intervalo se ajusta de acuerdo con la distribución de esta varianza bajo el supuesto de normalidad.

-   **Ejemplo:** Supongamos que un investigador mide los tiempos de espera en una fila y obtiene una varianza muestral de (s\^2 = 16) minutos² con una muestra de 15 observaciones. Para calcular un intervalo de confianza del 95% para la varianza poblacional, se utiliza la fórmula:

$$ IC = \left(\frac{(n-1)s^2}{\chi^2_{\alpha/2}}, \frac{(n-1)s^2}{\chi^2_{1-\alpha/2}}\right) $$

Donde (n) es el tamaño de la muestra y (\chi\^2) son los valores críticos de la distribución chi-cuadrado. Si (n=15) y con un nivel de confianza del 95%, se pueden encontrar los valores críticos:

$$ IC = \left(\frac{(15-1) \times 16}{\chi^2_{0.025, 14}}, \frac{(15-1) \times 16}{\chi^2_{0.975, 14}}\right) $$

Si $\chi^2_{0.025, 14} \approx 27.688$ y $\chi^2_{0.975, 14} \approx 5.629$:

$$ IC \approx \left(\frac{14 \times 16}{27.688}, \frac{14 \times 16}{5.629}\right) \approx (8.520, 39.658) $$

Esto significa que el investigador puede estar 95% seguro de que la varianza del tiempo de espera en la población está entre 8.520 y 39.658 minutos².

### Aplicación interactiva

Para ilustrar la idea de estimación por intervalo, puedes utilizar simulaciones interactivas que permiten explorar estos conceptos. Aquí tienes un recurso que permite interactuar con este concepto:

[Estimación por intervalo](https://seeing-theory.brown.edu/frequentist-inference/)

### Consistencia

Un **intervalo de confianza** es considerado **consistente** si, al aumentar el tamaño de la muestra, la longitud del intervalo tiende a reducirse. Esto indica que la estimación se vuelve más precisa y refleja mejor el parámetro poblacional.

### Construcción de los intervalos de confianza

La construcción de intervalos de confianza puede realizarse de dos maneras:

  1.  **Método analítico**: Utiliza fórmulas y propiedades matemáticas para calcular los intervalos de confianza. Este método es eficiente cuando se cumplen las condiciones necesarias, como la normalidad de los datos.

  2.  **Método computacional**: Utiliza técnicas de remuestreo, como el bootstrapping, para estimar la distribución del estimador y generar intervalos de confianza sin necesidad de asumir una distribución específica.

### Ventajas y desventajas de la construcción analítica

La construcción analítica de los intervalos de confianza presenta pros y contras

  -   **Ventajas**:
    -   Menor costo computacional, ya que implica cálculos directos a partir de los datos.
    -   Resultados más rápidos si se cumplen los supuestos de normalidad.

  -   **Desventajas**:
    -   Dependencia de supuestos que pueden no cumplirse en datos reales.
    -   Puede no ser adecuado para distribuciones no estándar o en situaciones de muestras pequeñas.

### Ventajas y desventajas de la construcción computacional

La construcción computacional de los intervalos de confianza presenta pros y contras


  -   **Ventajas**:
    -   Flexible, ya que no depende de supuestos sobre la distribución de los datos.
    -   Adecuado para cualquier tipo de muestra y variabilidad en los datos.

  -   **Desventajas**:
    -   Puede ser intensivo en recursos computacionales, especialmente para grandes volúmenes de datos.
    -   Mayor tiempo de procesamiento y necesidad de software especializado.

### Fórmulas para algunos intervalos

  -   **Intervalo de confianza para la media**:

$$IC = \bar{x} \pm z\_{\alpha/2} \left(\frac{\sigma}{\sqrt{n}}\right) \quad \text{(si } \sigma \text{ es conocido)}$$

$$IC = \bar{x} \pm t\_{\alpha/2} \left(\frac{s}{\sqrt{n}}\right) \quad \text{(si } \sigma \text{ es desconocido)}$$

  -   **Intervalo de confianza para la proporción**:

$$IC = \hat{p} \pm z\_{\alpha/2} \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}$$

  -   **Intervalo de confianza para la varianza**:

$$ IC = \left(\frac{(n-1)s^2}{\chi^2_{\alpha/2}}, \frac{(n-1)s^2}{\chi^2_{1-\alpha/2}}\right) $$

## Bootstrapping

El **bootstrapping** es un método no paramétrico que se utiliza para estimar la distribución de un estimador y, por ende, sus intervalos de confianza. Este método genera múltiples muestras simuladas (con reemplazo) de la muestra original, permitiendo construir un intervalo de confianza sin depender de la distribución subyacente de los datos.

### Ejemplo: Diferencia de medias

Para evaluar la **diferencia de medias** entre dos poblaciones, se construye un intervalo de confianza alrededor de la diferencia de medias muestrales. Dependiendo de si se asumen varianzas iguales o diferentes, se pueden aplicar diferentes métodos, como el uso de la distribución t de Student o el método de bootstrapping.

  - **Ejemplo**: Supongamos que queremos comparar las alturas promedio de hombres y mujeres en una población. Recogemos datos de 50 hombres y 50 mujeres y encontramos que la media de los hombres es 1.80 metros y la media de las mujeres es 1.65 metros. La diferencia de medias es 0.15 metros. Aplicando el bootstrapping, podemos crear múltiples muestras de nuestras muestras originales para estimar la distribución de la diferencia de medias y, así, construir un intervalo de confianza.

### Ejemplo: Diferencia de proporciones

El **intervalo de confianza para la diferencia de proporciones** entre dos poblaciones se construye de manera similar al de una proporción, pero utilizando la diferencia entre las proporciones muestrales. Dependiendo de si se asume o no la homogeneidad de varianzas entre las dos proporciones, se pueden aplicar diferentes métodos.

  - **Ejemplo**: Si en una encuesta a 300 hombres, el 70% responde que prefiere el trabajo remoto, y en una encuesta a 250 mujeres, el 60% responde lo mismo, la diferencia de proporciones es 0.10. Para construir un intervalo de confianza, podemos aplicar el bootstrapping para simular la diferencia de proporciones y calcular el intervalo.

### Bootstrapping ejemplo

Para explorar el método de bootstrapping, puedes interactuar con simulaciones que permiten visualizar cómo funciona este proceso en la práctica. Aquí tienes un recurso que permite interactuar con este concepto:

[Bootstrapping](https://seeing-theory.brown.edu/frequentist-inference/)

## Tamaño muestral

El **tamaño muestral** es un aspecto crucial en el diseño de estudios estadísticos, ya que influye directamente en la precisión y confiabilidad de las estimaciones. Comprender cómo determinar el tamaño de muestra adecuado es fundamental para obtener resultados válidos y aplicables a una población más amplia.

### Margen de error

El **margen de error** es una medida que refleja la cantidad de incertidumbre asociada a una estimación puntual. Se define como la mitad de la longitud de un intervalo de confianza y representa la variabilidad esperada en una estimación debido al muestreo. Cuanto mayor sea el margen de error, menos confiable será la estimación.

**Cálculo del margen de error:**

El margen de error se puede calcular utilizando la siguiente fórmula:

$$\text{Margen de error} = Z \times \left(\frac{\sigma}{\sqrt{n}}\right)$$

Donde:
- $Z$ es el valor crítico de la distribución normal estándar (por ejemplo, 1.96 para un nivel de confianza del 95%).
- $\sigma$ es la desviación estándar de la población (o de la muestra si la desviación estándar de la población no está disponible).
- $n$ es el tamaño de la muestra.

#### Ejemplo:

- **Encuesta electoral:** En una encuesta a 1,000 votantes, el 48% afirma que votará por el candidato A. Si el margen de error es de ±3%, esto indica que el porcentaje real de votantes que apoyan al candidato A se estima que está entre el 45% y el 51%. Esto se puede expresar como un intervalo de confianza de (0.45, 0.51).

- **Estimación de ingresos:** Se estima que el ingreso promedio de una población es de \$50,000 con un margen de error de ±\$2,000. Esto implica que el ingreso promedio real en la población se espera que esté entre \$48,000 y \$52,000. En este caso, el margen de error nos proporciona una indicación clara de la posible variabilidad en los ingresos de la población.

### Empate técnico

El **empate técnico** ocurre cuando las diferencias entre dos estimaciones puntuales, como medias o proporciones, no son estadísticamente significativas debido al margen de error. En este caso, las diferencias observadas podrían ser el resultado de la variabilidad muestral y no reflejan una verdadera diferencia en la población.

#### Ejemplo:

- **Encuesta electoral:** En una encuesta a 1,200 votantes, el candidato A obtiene el 46% de las intenciones de voto y el candidato B el 44%, con un margen de error de ±3%. Dado que el margen de error abarca los porcentajes de ambos candidatos (43% a 49% para A y 41% a 47% para B), se considera un empate técnico. Esto sugiere que no hay suficiente evidencia para afirmar que uno de los candidatos es preferido sobre el otro.

- **Competencia de ventas entre productos:** En un análisis de ventas, el producto X alcanza el 35% de participación en el mercado y el producto Y el 33%, con un margen de error de ±2%. Aquí, el margen de error (33% a 37% para X y 31% a 35% para Y) se superpone, lo que lleva a declarar un empate técnico entre los productos. Este tipo de análisis es crucial en la investigación de mercados, donde decisiones de marketing deben basarse en resultados estadísticamente significativos.

## Tamaño de muestra

El **tamaño de muestra** afecta directamente la precisión de las estimaciones. Un mayor tamaño de muestra generalmente reduce el error estándar, disminuye el margen de error y mejora la confiabilidad de las estimaciones. Sin embargo, también implica un aumento en los costos y el tiempo de recolección de datos.

### Tamaño de muestra óptimo

El tamaño de muestra óptimo se calcula bajo el supuesto de que cada registro genera un costo. En este sentido, la optimización se realiza disminuyendo el tamaño muestral con la restricción de que las estimaciones sean estadísticamente distintas de cero.

**Consideraciones para determinar el tamaño óptimo:**

1. **Costos:** Es fundamental considerar los costos asociados a la recolección de datos. Un tamaño muestral mayor a lo necesario puede generar costos innecesarios, mientras que un tamaño menor puede resultar en una estimación menos útil.

2. **Variabilidad:** La variabilidad dentro de la población impacta la determinación del tamaño de la muestra. Si la población es altamente variable, se necesitará un tamaño de muestra mayor para capturar esta variabilidad.

3. **Nivel de confianza y margen de error deseados:** Un mayor nivel de confianza o un margen de error más pequeño requerirá un tamaño de muestra más grande. Por ejemplo, si deseamos un nivel de confianza del 99% en lugar del 95%, el tamaño de la muestra tendrá que aumentar.

4. **Tamaño poblacional:** En poblaciones muy grandes, la relación entre el tamaño de la muestra y el tamaño poblacional es menos crítica. Sin embargo, en poblaciones pequeñas, se debe considerar el efecto de la muestra sobre el total.


### Fórmulas para Estimación

#### 1. Estimación de una Proporción con Tamaño Poblacional Conocido

Cuando se conoce el tamaño de la población y se desea estimar una proporción, se puede utilizar la siguiente fórmula:

$$n = \frac{N \cdot Z^2 \cdot p(1 - p)}{(N - 1) \cdot E^2 + Z^2 \cdot p(1 - p)}$$

Donde:
- $n$ = tamaño de la muestra
- $N$ = tamaño de la población
- $Z$ = valor crítico de la distribución normal (por ejemplo, 1.96 para un nivel de confianza del 95%)
- $p$ = proporción estimada de la población (por ejemplo, 0.5 si no se conoce)
- $E$ = margen de error deseado

#### 2. Estimación de una Proporción con Tamaño Poblacional Desconocido

Cuando el tamaño de la población es desconocido, se utiliza la siguiente fórmula simplificada:

$$n = \frac{Z^2 \cdot p(1 - p)}{E^2}$$

Donde:
- $n$ = tamaño de la muestra
- $Z$ = valor crítico de la distribución normal (por ejemplo, 1.96 para un nivel de confianza del 95%)
- $p$ = proporción estimada de la población (puede ser 0.5 si se desea máxima variabilidad)
- $E$ = margen de error deseado

#### 3. Estimación de una Media con Tamaño Poblacional Conocido

Si el tamaño de la población es conocido y se desea estimar la media, se utiliza la siguiente fórmula:

$$n = \frac{N \cdot Z^2 \cdot \sigma^2}{(N - 1) \cdot E^2 + Z^2 \cdot \sigma^2}$$

Donde:
- $n$ = tamaño de la muestra
- $N$ = tamaño de la población
- $Z$ = valor crítico de la distribución normal (por ejemplo, 1.96 para un nivel de confianza del 95%)
- $\sigma$ = desviación estándar de la población
- $E$ = margen de error deseado

#### 4. Estimación de una Media con Tamaño Poblacional Desconocido

Cuando el tamaño de la población es desconocido, se utiliza la siguiente fórmula:

$$n = \frac{Z^2 \cdot \sigma^2}{E^2}$$

Donde:

- $n$ = tamaño de la muestra
- $Z$ = valor crítico de la distribución normal (por ejemplo, 1.96 para un nivel de confianza del 95%)
- $\sigma$ = desviación estándar de la población (o de la muestra si es necesario)
- $E$ = margen de error deseado


Es importante considerar que, al utilizar estas fórmulas, se deben tener en cuenta supuestos como la normalidad de la población y la aleatoriedad de la muestra. Además, los valores de $p$ y $\sigma$ deben ser estimados de forma adecuada para obtener resultados confiables. Para poblaciones pequeñas, es recomendable aplicar una corrección de población finita si se utiliza la fórmula con el tamaño poblacional conocido.

Estas fórmulas son herramientas esenciales en la investigación estadística y permiten a los investigadores diseñar estudios que produzcan estimaciones precisas y confiables.

#### Ejemplo:

- En un estudio sobre hábitos de consumo, se determina que el costo de encuestar a un individuo es de \$10. Si se quiere estimar el gasto promedio mensual con un margen de error de ±\$5 y un nivel de confianza del 95%, se puede calcular el tamaño de muestra necesario utilizando la fórmula del tamaño de muestra:

$$n = \left(\frac{Z^2 \cdot \sigma^2}{E^2}\right)$$

Donde:

- $E$ es el margen de error deseado.

Al determinar que la desviación estándar de los gastos mensuales es \$50, se puede calcular el tamaño muestral óptimo para cumplir con los criterios de estudio.

- **Costo-beneficio:** Supongamos que al calcular el tamaño muestral óptimo se determina que es de 100 encuestas. Si se decide llevar a cabo 150 encuestas, aunque se logra una mayor precisión, el costo adicional debe justificarse por los beneficios esperados del estudio.

En resumen, un tamaño muestral mayor al óptimo produce estimaciones que son distintas de cero, pero incurre en costos innecesarios. Por otro lado, un tamaño muestral menor al óptimo produce estimaciones que son estadísticamente iguales a cero, disminuyendo la utilidad del ejercicio. Por lo tanto, encontrar un equilibrio entre precisión y costo es esencial en el diseño de estudios estadísticos.


### Aplicación Interactiva

Para ayudar a comprender cómo calcular el tamaño de muestra, se puede utilizar la siguiente herramienta interactiva:

[Cálculo del tamaño de muestra](http://gauss.medellin.unal.edu.co:3838/fhernanb/samplesize/)

Esta aplicación te permite ingresar parámetros como el nivel de confianza, el margen de error, y la proporción esperada o la desviación estándar, dependiendo de si deseas estimar una proporción o una media. La herramienta calculará automáticamente el tamaño de muestra necesario para tu estudio, facilitando así la planificación y diseño de investigaciones.

**Instrucciones:**

1. Accede al enlace proporcionado.
2. Selecciona el tipo de estimación que deseas realizar (proporción o media) y (si lo conoces) el temaño poblacional.
3. Ingresa el margen de error que necesitas.
4. Haz clic en "Calcular" para obtener el tamaño de muestra recomendado.

Esta aplicación es especialmente útil para investigadores y profesionales que desean asegurarse de que sus estimaciones sean precisas y confiables, optimizando así los recursos destinados a la recolección de datos.


# Revisar!!

¡Vamos a profundizar aún más en cada sección, incorporando más detalles sobre los autores mencionados y su relevancia en el contexto de las pruebas de hipótesis y la epistemología!

# Pruebas de hipótesis

## La hipótesis en el contexto del desarrollo científico

Epistemológicamente, una **hipótesis** es una proposición que establece una relación entre variables, formulada para ser sometida a prueba a través de la observación y la experimentación. Este proceso es esencial en el desarrollo científico, ya que las hipótesis funcionan como un puente entre la teoría y los datos empíricos. Una hipótesis permite a los investigadores establecer un marco para la investigación, formulando preguntas que pueden ser respondidas mediante métodos científicos. Al plantear una hipótesis, el investigador no solo está formulando una afirmación, sino que también está invitando a la crítica y la validación a través de experimentos y observaciones.

Las pruebas de hipótesis son fundamentales en este proceso, ya que permiten evaluar la validez de las afirmaciones mediante un enfoque sistemático y basado en datos. Este enfoque no solo fomenta la acumulación de conocimiento, sino que también contribuye a la evolución de teorías dentro de un campo determinado. La capacidad de una hipótesis para ser refutada o confirmada es lo que la hace esencial para el avance del conocimiento científico, manteniendo siempre un carácter provisional y sujeto a revisión.

## Positivismo: Comte y ...

El **positivismo**, defendido por Auguste Comte, establece que el conocimiento debe basarse en la observación y la medición. Comte propuso que la ciencia debía evolucionar en tres etapas: teológica, metafísica y positiva. En la etapa positiva, se espera que el conocimiento se base exclusivamente en hechos observables y datos cuantitativos. 

- **Medición estricta:** Este enfoque enfatiza la importancia de la recolección de datos precisos y cuantificables. Comte sostenía que el progreso del conocimiento científico era posible solo a través de una medición rigurosa. Por lo tanto, las hipótesis deben ser formuladas de manera que se puedan poner a prueba mediante la observación directa y la experimentación.

- **El dato construye el relato:** En esta perspectiva, los datos empíricos son la base sobre la cual se construyen las teorías científicas. Se sostiene que la interpretación de los fenómenos debe derivar de los datos y no al revés. Esto implica que los científicos deben ser cautelosos al formular afirmaciones sin un respaldo empírico sólido.

- **Mido, pienso, afirmo:** Este mantra encapsula el proceso positivista en la investigación científica. El investigador primero mide (recoge datos), luego piensa (formula hipótesis y teorías) y finalmente afirma (hace conclusiones basadas en los datos).

## Verificacionismo

El **verificacionismo** es una postura epistemológica que sostiene que una afirmación científica debe ser verificable para ser considerada válida. Este enfoque, influenciado por el empirismo lógico, establece que el conocimiento se deriva de la experiencia y la observación. 

- **Medición:** La medición es fundamental en este enfoque, ya que el proceso de verificación se basa en datos concretos que pueden ser observados y medidos. La validez de las afirmaciones científicas se establece a través de pruebas empíricas que corroboran las hipótesis iniciales.

- **Hacer pruebas buscando verificar las afirmaciones:** Los investigadores buscan activamente evidencia que confirme sus hipótesis, lo que lleva a un ciclo de prueba y verificación. En este sentido, el enfoque verificacionista refuerza la credibilidad de las teorías científicas.

- **Buscar cisnes:** David Hume argumentó que no podemos probar la universalidad de una afirmación a través de la observación de casos particulares. En otras palabras, observar muchos cisnes blancos no significa que no existan cisnes negros. Este punto resalta la limitación del verificacionismo y sugiere que las afirmaciones científicas deben ser revisadas continuamente ante nuevas evidencias.

## Falsacionismo

El **falsacionismo**, desarrollado por Karl Popper, presenta una crítica al verificacionismo y establece que la ciencia avanza mediante la **refutación** de teorías en lugar de su verificación. 

- **Afirmo, compruebo lo contrario, refuto:** Este enfoque implica que, para que una teoría sea científica, debe ser susceptible de ser refutada. Las hipótesis deben formularse de tal manera que puedan ser sometidas a pruebas que podrían demostrar su falsedad.

- **Ingenuo vs. completo:** Las teorías pueden ser clasificadas como ingenuas, que admiten excepciones y realizan justificaciones sobre la teoría actual, o completas, que no permiten excepciones y propugnan por nuevas teorías que ofrecen explicaciones más adecuadas.

- **Lakatos y los programas científicos:** Imre Lakatos expandió las ideas de Popper, sugiriendo que las teorías se desarrollan en el contexto de programas científicos que contienen un "núcleo duro" que es protegido por una serie de teorías auxiliares. Este enfoque permite que algunas teorías se mantengan incluso ante evidencias contradictorias, siempre que las justificaciones sean coherentes y no arbitrarias.

- **Fleck y la construcción del conocimiento:** Ludwik Fleck introdujo la idea de que el conocimiento científico se desarrolla en "estilos de pensamiento", que son influidos por las comunidades científicas. Esto implica que la interpretación de datos y teorías está contextualizada por el paradigma dominante en un momento dado.

- **Kuhn y los paradigmas científicos:** Thomas Kuhn argumentó que la ciencia avanza a través de **paradigmas**, que son marcos conceptuales ampliamente aceptados que guían la investigación en un campo. Kuhn postuló que la ciencia no progresa de manera lineal, sino a través de revoluciones científicas donde un paradigma es reemplazado por otro, a menudo después de una crisis de confianza en el paradigma anterior.

## Popper dice

Karl Popper formuló una serie de principios que resumen su perspectiva sobre el conocimiento científico y las pruebas de hipótesis:

1. Es fácil obtener confirmaciones para casi cualquier teoría, si buscamos confirmaciones.
2. Las confirmaciones deben contar solo si son el resultado de predicciones arriesgadas; es decir, si, sin conocimiento previo, deberíamos haber esperado un evento que sea incompatible con la teoría.
3. Cada “buena” teoría científica es una prohibición: prohíbe ciertos eventos. Cuanto más prohíbe una teoría, mejor es.
4. Una teoría que no puede ser refutada por ningún evento concebible es no científica; la irrefutabilidad no es una virtud de una teoría, sino un vicio.
5. Cada prueba genuina de una teoría es un intento de falsificarla. La testabilidad es falsabilidad; hay grados de testabilidad, algunas teorías son más testables que otras.
6. La evidencia confirmadora no debe contar excepto cuando resulta de una prueba genuina de la teoría; y esto significa que puede ser presentada como un intento serio pero fallido de falsificar la teoría.
7. Algunas teorías genuinamente comprobables, cuando se encuentran falsas, son aún defendidas por sus admiradores, introduciendo supuestos auxiliares ad hoc o reinterpretando la teoría ad hoc para evitar la refutación. Tal procedimiento rescata la teoría de la refutación solo a costa de destruir, o al menos disminuir, su estatus científico.

Popper concluye que el criterio del estatus científico de una teoría es su **falsabilidad**, **refutabilidad** o **testabilidad**.

## Paradigma

El concepto de **paradigma** se refiere a un conjunto de creencias, valores y técnicas compartidos por una comunidad científica que guían la investigación. Los paradigmas no solo establecen qué preguntas son relevantes, sino que también determinan qué métodos son apropiados para abordarlas. 

- **No es tan importante saber, sino tener una manera de saber si sabes:** Esta afirmación destaca la importancia de la metodología en la ciencia. La forma en que se estructuran las preguntas de investigación y se diseñan los experimentos es esencial para el progreso del conocimiento científico.

- **Los programas científicos:** La idea de los programas científicos se refiere a las teorías que evolucionan y se sostienen a través de un núcleo duro de conceptos fundamentales, que son protegidos por teorías auxiliares. Esto permite a los científicos trabajar dentro de un marco coherente mientras mantienen la capacidad de cuestionar y modificar teorías a medida que surgen nuevos datos.

- **Los paradigmas científicos:** Los paradigmas establecen un contexto en el que se desarrolla la investigación y que, a su vez, influye en la interpretación de los resultados. Este enfoque sugiere que la ciencia es un proceso dinámico y en evolución, donde la crítica y la revisión son esenciales para el avance.

## Qué es la reproducibilidad y por qué es importante

La **reproducibilidad** es un principio fundamental en la ciencia, que implica que los resultados de un experimento deben poder ser replicados por otros investigadores utilizando los mismos métodos. Esto no solo fortalece la validez de los hallazgos, sino que también asegura que el conocimiento científico se basa en evidencias sólidas y verificables.

- **Importancia:** La reproducibilidad es esencial para la credibilidad de la ciencia y se convierte en una medida del rigor y la fiabilidad de los estudios realizados. Sin reproducibilidad, los hallazgos pueden ser considerados anecdóticos y su impacto en la ciencia puede ser severamente cuestionado.

## Reproductibilidad

La **crisis actual de la reproducibilidad** en la ciencia se refiere a la creciente preocupación sobre la capacidad de replicar resultados científicos. Estudios han demostrado que un número significativo de resultados

 en diversas disciplinas no puede ser replicado, lo que plantea interrogantes sobre la validez de esos hallazgos. 

- **Los escándalos en la ciencia:** Casos de fraude, mala conducta científica y prácticas poco éticas han salido a la luz, socavando la confianza en la investigación. Esto ha llevado a un llamado a la transparencia en los métodos y datos utilizados, así como a la implementación de mejores prácticas para asegurar la reproducibilidad.

## Conclusiones

La discusión sobre las pruebas de hipótesis y su relación con la epistemología revela la complejidad del conocimiento científico. A través de la evolución del pensamiento científico, desde el positivismo hasta el falsacionismo y el paradigma de Kuhn, se ha evidenciado que la ciencia es un proceso dinámico y crítico, donde la validación y la crítica son esenciales para el progreso.

Es fundamental entender que la formulación de hipótesis no es un mero ejercicio teórico, sino una actividad que se encuentra en el corazón de la investigación científica. La capacidad de probar y refutar teorías permite a la ciencia avanzar, cuestionar lo establecido y adaptarse a nuevos descubrimientos. A medida que enfrentamos la crisis de la reproducibilidad, es más importante que nunca reforzar la ética en la investigación, promover la transparencia y fomentar un enfoque crítico hacia el conocimiento científico.


# Aquí vamos!!


## Probabilidad

Es posible ver la probabilidad como el estudio del comportamiento de los datos generados a partir de una distribución conocida.

## Estadística

Análogamente, la estadística es el estudio del comportamiento de las distribuciones asociadas a un conjunto de datos dado.

## Modelo estadístico

Un modelo estadístico es un conjunto de supuestos matemáticos que se realizan sobre la distribución asociada a un conjunto de datos.


## Objetivo

Establecer si existe **suficiente evidencia** en una muestra aleatoria para **rechazar** o **no rechazar** la **hipótesis nula** a nivel poblacional. El objetivo es determinar cuál hipótesis explica mejor los datos observados en la población.

## Problema de la arquitectura de las pruebas de hipótesis

**Pregunta clave:** ¿Los videojuegos violentos causan comportamientos violentos?

Este problema ejemplifica cómo se estructura una prueba de hipótesis: planteando una hipótesis nula (que niega una relación) y una alternativa (que afirma una relación), y evaluando cuál de estas hipótesis está mejor apoyada por los datos.

## Terminología

-   **Hipótesis estadística**: Una aseveración sobre uno o más parámetros poblacionales que puede ser probada mediante datos muestrales.
-   **Sistema de hipótesis**: Conjunto de hipótesis nula y alternativa que se contrastan.
-   **Tipos de error**: Los posibles errores al tomar una decisión basada en una prueba de hipótesis.
-   **Nivel de significancia**: La probabilidad de cometer un error tipo 1, es decir, rechazar la hipótesis nula cuando en realidad es cierta.

## Hipótesis estadística

Una **hipótesis estadística** es una afirmación sobre un parámetro poblacional. Puede ser:

-   **hipótesis simple**: Especifica un valor único para el parámetro, por ejemplo, $H:\mu = \mu_0$.
-   **hipótesis compuesta**: Especifica un rango de valores posibles para el parámetro, por ejemplo, $H:\mu \geq \mu_0$, $H:\mu \leq \mu_0$, $H:\mu > \mu_0$, $H:\mu < \mu_0$, $H:\mu \neq \mu_0$.

El valor $\mu_0$ es el **valor hipotético** contra el cual se compara la estimación muestral.

## Sistema de hipótesis

El **sistema de hipótesis** consiste en dos hipótesis contrapuestas:

-   **hipótesis nula (**$H_0$): Es la hipótesis que se presume cierta inicialmente. Ejemplos:
    -   $H_0:\mu \leq \mu_0$
    -   $H_0:\mu \geq \mu_0$
    -   $H_0:\mu = \mu_0$
-   **hipótesis alternativa (**$H_1$): Es la hipótesis que se acepta si los datos proporcionan suficiente evidencia para rechazar $H_0$. Ejemplos:
    -   $H_1:\mu > \mu_0$
    -   $H_1:\mu < \mu_0$
    -   $H_1:\mu \neq \mu_0$

## Tipos de error

En una prueba de hipótesis, pueden ocurrir dos tipos de errores.

-   **error tipo 1**: Rechazar la hipótesis nula cuando esta es verdadera. La probabilidad de cometer este error es el nivel de significancia $\alpha$.
-   **error tipo 2**: No rechazar la hipótesis nula cuando esta es falsa. La probabilidad de cometer este error se denota como $\beta$.

## Tipos de error

En una prueba de hipótesis, pueden ocurrir dos tipos de errores.

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRuldWSenU1lKPw2F7kkKH5zraE5wHibmAdM21m88UDZDN1Jvqb)

## Nivel de significancia

El **nivel de significancia** ($\alpha$) es la probabilidad de cometer un error tipo 1, es decir, rechazar la hipótesis nula cuando en realidad es cierta. Este nivel se establece antes de realizar la prueba y comúnmente se fija en 0.05 o 0.01.

-   $$\alpha = P(\text{error tipo 1}) = P(\text{rechazar $H_0$}\mid H_0)$$

## Rechazar la hipótesis nula

Rechazar $H_0$ implica que se ha encontrado algo en la muestra tan improbable bajo la hipótesis nula, que lleva al investigador a favorecer la hipótesis alternativa $H_1$. Sin embargo, siempre existe la posibilidad de cometer un error tipo 1 al hacer esta decisión.

### La ciencia es conservadora

En estadística, se prefiere no rechazar $H_0$ erróneamente a rechazarla sin suficiente evidencia. Por esto, $H_0$ se mantiene a menos que haya **evidencia contundente** que obligue a revocarla. Este enfoque refleja un principio conservador en la ciencia, donde se requiere una alta carga de prueba para cambiar el estado actual del conocimiento.

## Valor p

El **valor** $p$ es la probabilidad de observar un estadístico de prueba tan extremo o más extremo que el observado, bajo la suposición de que la hipótesis nula es cierta.

-   Si $p < \alpha$, se rechaza $H_0$, lo que sugiere que la evidencia muestral no es compatible con $H_0$.
-   Si $p \geq \alpha$, no se rechaza $H_0$, indicando que los datos son consistentes con $H_0$.

Es importante recordar que el valor $p$ no mide la probabilidad de que $H_0$ sea verdadera, sino la probabilidad de los datos observados bajo $H_0$.

## Decisión

La decisión en una prueba de hipótesis se basa en comparar el valor $p$ con el nivel de significancia $\alpha$:

-   **Si** $p < \alpha$, se **rechaza** $H_0$.
-   **Si** $p \geq \alpha$, **no se rechaza** $H_0$.

Esta decisión refleja si la evidencia contra $H_0$ es lo suficientemente fuerte para considerarla improbable bajo su supuesta veracidad.

## Prueba

El proceso para realizar una prueba de hipótesis incluye los siguientes pasos:

1.  **Establecer las hipótesis**: Definir $H_0$ y $H_1$ basándose en el problema de investigación.
2.  **Formular el sistema de hipótesis** y seleccionar el nivel de significancia $\alpha$.
3.  **Calcular el valor** $p$: Utilizar los datos muestrales para calcular el estadístico de prueba y el correspondiente valor $p$.
4.  **Tomar la decisión**: Rechazar $H_0$ si $p < \alpha$ o no rechazar $H_0$ si $p \geq \alpha$.
5.  **Interpretar los resultados**: Explicar el resultado en el contexto del problema de investigación, indicando si hay evidencia suficiente para apoyar $H_1$.

## Pruebas paramétricas y no paramétricas

### Pruebas paramétricas

Las **pruebas paramétricas** estudian un parámetro de dimensión finita (una colección finita de parámetros), por esta razón presentan supuestos específicos sobre la distribución de los datos.

### Pruebas no paramétricas

Las **pruebas paramétricas** estudian un parámetro de dimensión infinita (funciones de densidad).

**Analíticas**: Hacen uso de propiedades generales de las distribuciones.

**Simulación estocástica**: Hacen uso de simulación computacional para obtener el valor-p.

## Pruebas paramétricas

Asumen que los datos siguen una distribución conocida, usualmente normal. Estas pruebas requieren que se cumplan ciertos supuestos, como la homogeneidad de varianzas y la linealidad.

Estas pruebas son más potentes cuando los datos cumplen estos supuestos, ya que utilizan toda la información disponible en los datos.

Cuando los datos no cumplen los supuestos, las pruebas pierden validez y los estudios rigor científico.

-   Prueba t de Student (para la comparación de medias)
-   ANOVA (para la comparación de medias entre múltiples grupos)

## Pruebas no paramétricas analíticas

Las **pruebas no paramétricas analíticas** hacen uso de propiedades matemáticas generales de las distribuciones continuas. Por lo tanto, no requieren supuestos tan fuertes sobre la distribución de los datos.

Son útiles cuando los datos no cumplen los supuestos necesarios para las pruebas paramétricas, como en casos de datos con distribuciones no normales o escalas de medición ordinales.

Son menos potentes que las pruebas paramétricas en los casos paramétricos. No obstante, son más flexibles y aplicables a una mayor variedad de situaciones.

-   Prueba de Mann-Whitney (para la comparación de dos grupos independientes)
-   Prueba de Wilcoxon (para la comparación de dos grupos pareados)
-   Prueba de Kruskal-Wallis (para la comparación de múltiples grupos)

## Pruebas no paramétricas de simulación estocástica

Las **pruebas de permutaciones** son métodos estadísticos no paramétricos que evalúan la significancia de un estadístico de prueba al permutar, simular o remuestrear los datos observados con el fin de obtener una muestra de estadísticos bajo H_0.

Al comparar el estadístico observado con la muestra de los estadísticos obtenidos, se puede determinar si el efecto observado es significativo.

-   **Permutación**: Se basa en la idea de que si no hay efecto, todas las permutaciones de los datos son igualmente probables.

-   **Simulación**: Se basa en la idea de que es posible simular la hipótesis nula y encontrar una muestra para el estimador bajo estas condiciones.

-   **Remuestreo**: Se basa en la idea de que una muestra aleatoria de una muestra aleatoria es una muestra aleatoria.

## Pruebas no paramétricas de simulación estocástica

Al comparar el estadístico observado con la distribución de los estadísticos obtenidos de las permutaciones, se puede determinar si el efecto observado es significativo.

**Calculo del valor p**: Se calcula como la proporción de permutaciones en las que el estadístico de prueba es al menos tan extremo como el observado.

Estas pruebas no dependen de supuestos sobre la distribución de los datos, lo que las hace muy flexibles y robustas frente a diversas condiciones de los datos.

## Cuándo usar cada tipo de prueba

Dependiendo de la cantidad de conocimiento que tenemos sobre los datos podemos elegir cuándo aplicar cada tipo de prueba.

-   **Pruebas paramétricas**: cuando los datos cumplen con los supuestos de normalidad y homogeneidad de varianzas, y se requiere mayor potencia estadística.

-   **Pruebas no paramétricas**: cuando los datos no cumplen con los supuestos paramétricos o cuando se trabaja con datos ordinales o no numéricos. Son útiles sean analíticas o de simulación estocástica.

# Sesión 4: Pruebas de hipótesis (dos poblaciones)

## Test t de Student para una muestra

En la familia normal, una muestra aleatoria $X = \{X_1, ... , X_n\}$ especificamos las hipótesis

$$
\begin{align*}
  H_0: \mu = \mu_0\\
  H_A: \mu \neq \mu_0
\end{align*}
$$

## Test de signos para una muestra

En la familia de distribuciones continuas, siendo $\theta$ la mediana, una muestra aleatoria $X = \{X_1, ... , X_n\}$ especificamos las hipótesis.

$$
\begin{align*}
  H_0: \theta = \theta_0\\
  H_A: \theta \neq \theta_0
\end{align*}
$$

## Test t de Student para dos muestras independientes

En la familia normal, dos muestras aleatorias $X = \{X_1, ... , X_m\}$ y $Y = \{Y_1, ... , Y_n\}$, bajo homogenedidad ($\sigma^2_x = \sigma^2_y$), especificamos las hipótesis

$$
\begin{align*}
  H_0: \mu_X = \mu_Y\\
  H_A: \mu_X \neq \mu_Y
\end{align*}
$$

## Test t de Student para dos muestras independientes

En la familia normal, dos muestras aleatorias $X = \{X_1, ... , X_m\}$ y $Y = \{Y_1, ... , Y_n\}$, bajo heterogenedidad ($\sigma^2_x \neq \sigma^2_y$), especificamos las hipótesis

$$
\begin{align*}
  H_0: \mu_X = \mu_Y\\
  H_A: \mu_X \neq \mu_Y
\end{align*}
$$

## Test Wilcoxon - Mann - Whitney

En la familia de las distribuciones continuas, siendo $\theta$ la mediana, sean dos muestras aleatorias $X = \{X_1, ... , X_m\}$ y $Y = \{Y_1, ... , Y_n\}$, especificamos las hipótesis

$$
\begin{align*}
  H_0: \theta_X = \theta_Y\\
  H_A: \theta_X \neq \theta_Y
\end{align*}
$$

## Test t de Student para dos muestras pareadas

En la familia normal, dos muestras aleatorias pareadas $X = \{X_1, ... , X_n\}$ y $Y = \{Y_1, ... , Y_n\}$, bajo heterogenedidad ($\sigma^2_x \neq \sigma^2_y$), especificamos las hipótesis

$$
\begin{align*}
  H_0: \mu_X = \mu_Y\\
  H_A: \mu_X \neq \mu_Y
\end{align*}
$$

Es suficiente ver que si $D = X - Y$ entonces podemos probar

$$
\begin{align*}
  H_0: \mu_D = 0\\
  H_A: \mu_D \neq 0
\end{align*}
$$

## Prueba de una sola proporción

En una distribución Bernoulli($\pi$), sea una muestra $X = \{X_1, ... , X_n\}$, especificamos las hipótesis

$$
\begin{align*}
  H_0: \pi_X = \pi_0\\
  H_A: \pi_X \neq \pi_0
\end{align*}
$$

## Prueba de dos proporciones

En una distribución Bernoulli($\pi$), sean dos muestras $X = \{X_1, ... , X_n\}$ y $Y = \{Y_1, ... , Y_n\}$, especificamos las hipótesis

$$
\begin{align*}
  H_0: \pi_X = \pi_y\\
  H_A: \pi_X \neq \pi_y
\end{align*}
$$

## Test de correlación de Pearson

En la familia normal, sea una muestra aleatoria de dos variables $X = \{X_1, ... , X_n\}$ y $Y = \{Y_1, ... , Y_n\}$ especificamos las hipótesis

$$
\begin{align*}
H_0: \rho =   \rho_0\\
H_A: \rho \neq   \rho_0
\end{align*}
$$

## Test de correlación de Spearman

En la familia de distribuciones continuas, sea una muestra aleatoria de dos variables $X = \{X_1, ... , X_n\}$ y $Y = \{Y_1, ... , Y_n\}$ especificamos las hipótesis

$$
\begin{align*}
H_0: \rho_s =   \rho_{s0}\\
H_A: \rho_s \neq   \rho_{s0}
\end{align*}
$$
